{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2f6579-19e3-41bd-8460-5ef9f5068658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db603ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "if os.path.exists('Y:/VIDEOS/1119/Videos/'):\n",
    "    print('Y:/VIDEOS/1119/Videos/ exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd58586",
   "metadata": {},
   "source": [
    "## Validation of Folders/Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ccdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "folder = 'Y:/VIDEOS'\n",
    "\n",
    "ids = os.listdir(folder)\n",
    "excluded_list = []\n",
    "for idx in ids:\n",
    "    \n",
    "    # check folder name\n",
    "    if os.path.exists(folder + '/' + idx + '/Videos/'):\n",
    "        print(f'idx: {idx}, folder: {folder + \"/\" + idx + \"/Videos/\"}')\n",
    "        files = glob.glob(folder + '/' + idx + '/Videos/*')\n",
    "    else:\n",
    "        # print(f'idx: {idx}, folder: {folder + \"/\" + idx + \"/Video/\"}')\n",
    "        files = glob.glob(folder + '/' + idx + '/**/*.asf')    \n",
    "\n",
    "    # check for 827 or 984\n",
    "    if os.path.exists(folder + '/' + idx + '/827000/') or os.path.exists(folder + '/' + idx + '/984/'):\n",
    "        print(f'idx: {idx}, 827 or 984')\n",
    "        excluded_list.append(idx)\n",
    "        continue\n",
    "    \n",
    "    # check for 827000 or 984 in nested folder\n",
    "    if os.path.exists(folder + '/' + idx + '/Video/827000/') or os.path.exists(folder + '/' + idx + '/Video/984/'):\n",
    "        print(f'idx: {idx}, 827 or 984')\n",
    "        excluded_list.append(idx)\n",
    "        continue \n",
    "\n",
    "    if len(files) < 1:\n",
    "        print(f'idx: {idx}, files: {len(files)}')\n",
    "        excluded_list.append(idx)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e633cf",
   "metadata": {},
   "source": [
    "### Validation for Empty Video Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d653d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "folder = 'Y:/VIDEOS'\n",
    "# video_list = glob.glob(folder + '/*/Video/**/*.asf')\n",
    "# video_list\n",
    "\n",
    "ids = os.listdir(folder)\n",
    "excluded_list = []\n",
    "for idx in ids:\n",
    "    files = glob.glob(folder + '/' + idx + '/Video/**/*.asf')\n",
    "    if len(files) < 1:\n",
    "        print(f'idx: {idx}, files: {len(files)}')\n",
    "        excluded_list.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_list = ['1009 1010',\n",
    "                 '1011 1012',\n",
    "                 '1003 1004',\n",
    "                 '1031 1017',\n",
    "                 '1011 1012',\n",
    "                 '1043 1044',\n",
    "                 '1055 1056',\n",
    "                 '1074 1075',\n",
    "                 '1099 1100',\n",
    "                 '1013 1014',\n",
    "                 '1003 1004-nonAI',\n",
    "                 '1005-nonAI',\n",
    "                 '1073',\n",
    "                 '1082',\n",
    "                 '1094',\n",
    "                 '1097',\n",
    "                 '2002',\n",
    "                 '2062'\n",
    "                 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672caf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import metaData\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd47c2d0-3e97-4738-9004-7192341864f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = 'Datafiles/data_storage_Jan2023.json'\n",
    "with open(json_file) as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b329e-eec5-4096-9999-dca66580f59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_list = {}\n",
    "days = []\n",
    "for i, d in enumerate(data['data'][:500]):\n",
    "    if d['id'] in days_list.keys():\n",
    "        days_list[d['id']].append(d['day'])\n",
    "    elif d['id'] not in days_list.keys():\n",
    "        days_list[d['id']] = list()\n",
    "        days_list[d['id']].append(d['day'])\n",
    "    # print(f\"Index: {i}, Data: {d['day']}\")\n",
    "    # days.append(d['day'])\n",
    "\n",
    "days_list_sorted = {k: sorted(v) for k, v in days_list.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c16497",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(json_file)\n",
    "dt = pd.DataFrame(df['data'], columns=['id', 'day', 'duration'])\n",
    "\n",
    "\n",
    "jn = pd.json_normalize(df['data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293fa3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = []\n",
    "ts_list = []\n",
    "for col, row in jn.iloc[:10].iterrows():\n",
    "    for idx in row:\n",
    "        # col_list.append(idx)\n",
    "        if type(idx) == list and len(idx) > 0:\n",
    "            for item in idx:\n",
    "                ts_list.append(item)\n",
    "        else:\n",
    "            col_list.append(idx)\n",
    "            if ts_list != []:\n",
    "                col_list.append(ts_list)\n",
    "                ts_list = []\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2302bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jn.columns\n",
    "\n",
    "ts_cols = ['yawn.timestamp',\n",
    "           'smoking.timestamp',\n",
    "           'mobilephone.timestamp',\n",
    "           'distraction.timestamp',\n",
    "           'eyeclosing.timestamp',\n",
    "           'crossinglane.timestamp',\n",
    "           'nearcollision.timestamp',\n",
    "           'stopsign.timestamp',\n",
    "           'redlight.timestamp',\n",
    "           'pedestrian.timestamp',\n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86f0e37",
   "metadata": {},
   "source": [
    "### Data Aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef44c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  sort a list of dictionaries by value1 and value2\n",
    "def sort_list_of_dict_by_value(list_of_dict, value1, value2):\n",
    "    return sorted(list_of_dict, key=lambda k: (k[value1], k[value2]))\n",
    "\n",
    "\n",
    "# get next 90 days from a given date\n",
    "# return a single date for next 90 days\n",
    "def get_next_90_days(start_date):\n",
    "    yyyy = start_date[:4]\n",
    "    mm = start_date[4:6]\n",
    "    dd = start_date[6:]\n",
    "\n",
    "    converted_date = datetime.datetime(int(yyyy), int(mm), int(dd))\n",
    "    next_months = converted_date + datetime.timedelta(days=90)\n",
    "    return str(next_months)[:10].replace('-', '')\n",
    "\n",
    "def get_next_90_days_data_from_sorted_list_of_dict(list_of_dict, value1, value2):\n",
    "    next_90_days = []\n",
    "    for i, d in enumerate(list_of_dict):\n",
    "        if i == len(list_of_dict)-1:\n",
    "            break\n",
    "        next_90_days.append(get_next_90_days(d[value2]))\n",
    "    return next_90_days\n",
    "\n",
    "# calculate number of day between two dates\n",
    "def days_between(d1, d2):\n",
    "    d1 = datetime.datetime.strptime(d1, \"%Y%m%d\")\n",
    "    d2 = datetime.datetime.strptime(d2, \"%Y%m%d\")\n",
    "    return abs((d2 - d1).days)\n",
    "\n",
    "# days_between('20200301', '20200102')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86273cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = sort_list_of_dict_by_value(data['data'], 'id', 'day')\n",
    "d = get_next_90_days_data_from_sorted_list_of_dict(sl, 'id', 'day')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e29d4e",
   "metadata": {},
   "source": [
    "### Extracting Data Based on Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cb3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filePath = 'Z:/VIDEOS/1010/Disk_files/debug/'\n",
    "def extract_indices_from_log(filePath,file):\n",
    "    alarmsDict = {'NOBODY': 0, 'LOOKING_DOWN': 0, 'SMOKING': 0, 'CALLING':0, 'LDW': 0, 'EYE_CLOSED': 0, 'LDW_R': 0, 'LDW_L':0, 'FCW':0, 'camera cover!':0, 'infrared block!': 0 }\n",
    "    alarmsTimeStamp = {'NOBODY': [], 'LOOKING_DOWN': [], 'SMOKING': [], 'CALLING':[], 'LDW': [], 'EYE_CLOSED': [], 'LDW_R': [], 'LDW_L':[], 'FCW':[], 'camera cover!':[], 'infrared block!': [] }\n",
    "    # alarmsDict = {'alarm_type 5': 0, 'alarm_type 4': 0, 'alarm_type 3': 0, 'alarm_type 1': 0, 'alarm_type 2': 0, 'alarm_type 17': 0, 'alarm_type 18': 0, 'alarm_type 27':0, 'alarm_type 16':0, 'alarm_type 9':0, 'alarm_type 7': 0 }\n",
    "    # alarmsTimeStamp = {'alarm_type 5': [], 'alarm_type 4': [], 'alarm_type 3': [], 'alarm_type 1': [], 'alarm_type 2': [], 'alarm_type 17': [], 'alarm_type 18': [], 'alarm_type 27':[], 'alarm_type 16':[], 'alarm_type 9':[], 'alarm_type 7': [] }\n",
    "    \n",
    "    error_files = []\n",
    "    # print(f\"Processing debug file {file}\")\n",
    "    with open(os.path.join(filePath, file), 'r') as logFile:\n",
    "        lines = logFile.read().splitlines()\n",
    "        for line in lines:\n",
    "            words = line.split(\" \")\n",
    "            \"\"\"\n",
    "            Retrieve alarm will be here\n",
    "            \"\"\"\n",
    "            for alarm in alarmsDict.keys():\n",
    "                if alarm in words:\n",
    "                    alarmsDict[alarm] += 1\n",
    "                    # below is date extraction so needs to be corrected****** --> currently is this -->[20210504-140826][03][baseIVS]\n",
    "                    try:\n",
    "                        timestamp = \"\".join(words[0].split('[')[1].split('-')).strip(']')\n",
    "                        alarmsTimeStamp[alarm].append(timestamp)\n",
    "                    except Exception:\n",
    "                        error_files.append(file)\n",
    "\n",
    "        # logFile.close()\n",
    "        # print(alarmsDict)\n",
    "    return alarmsDict, alarmsTimeStamp, error_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90d2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_list = ['1003 1004','1031 1017','1011 1012','1043 1044','1013 1014','1003 1004-nonAI', '1005-nonAI', '1073', '2062']\n",
    "ids = []\n",
    "for item in os.listdir('Z:/VIDEOS/'):\n",
    "    if item not in excluded_list:\n",
    "        ids.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16cfbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfile = []\n",
    "mapping = {'NOBODY': 0,\n",
    "           'LOOKING_DOWN': 1,\n",
    "           'SMOKING': 2, \n",
    "           'CALLING': 3, \n",
    "           'LDW': 5, \n",
    "           'EYE_CLOSED': 4, \n",
    "           'LDW_R': 5, \n",
    "           'LDW_L':5, \n",
    "           'FCW':6, \n",
    "           'camera cover!':0, \n",
    "           'infrared block!': 0 }\n",
    "    \n",
    "for each_driver in tqdm(ids):\n",
    "    logFolder = f'Z:/VIDEOS/{each_driver}/Disk_files/debug/'\n",
    "    for logfile in os.listdir(logFolder):\n",
    "        driverid = logFolder.split('/')[2]\n",
    "        \n",
    "        # if logfile.endswith('.log'):\n",
    "        alarmdict, alarmtimestamp, err = extract_indices_from_log(logFolder, logfile)\n",
    "        for key, value in alarmtimestamp.items():\n",
    "            for ts in value:\n",
    "                if key != 'NOBODY':\n",
    "                    id = driverid\n",
    "                    alarm_type = key\n",
    "                    date = ts[:9]\n",
    "                    \n",
    "                    transform_dict = {'id': id, 'date': date, 'timestamp': ts, 'type': mapping[alarm_type]}\n",
    "                    jsonfile.append(transform_dict)\n",
    "\n",
    "with open('./Datafiles/Timestamps_data.json', 'w') as jsonf:\n",
    "    json.dump(jsonfile, jsonf)    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Formatting the data to used in excel reprots\n",
    "import json\n",
    "\n",
    "with open('data_storage.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for each_item in data['data']:\n",
    "    each_item.update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacca8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_of_structure(src, participant_id):\n",
    "    \n",
    "    if os.path.exists(src+'/Video/827000'):\n",
    "        print(f'Folder Exists 827000 in {participant_id}')\n",
    "        # preparing_folder(src+f'/Video/827000')\n",
    "        \n",
    "    elif os.path.exists(src+'/Video/984'):\n",
    "        print(f'Folder Exists 984 in {participant_id}')\n",
    "        # preparing_folder(src+f'/Video/984')\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "All_drivers = os.listdir('Z:/VIDEOS/')\n",
    "for each_driver in All_drivers:\n",
    "    src = f'Z:/VIDEOS/{each_driver}'\n",
    "    validation_of_structure(src, each_driver)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fe2fb9a6",
   "metadata": {},
   "source": [
    "### Uploading Data to HPC Servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7b97bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "from distutils.dir_util import copy_tree\n",
    "\n",
    "\n",
    "\n",
    "# def transferFilesToServer(src, dest):\n",
    "#     # files = glob.glob(src, recursive=True)\n",
    "#     # for root, dirs, files in os.walk(src):\n",
    "#     #     root = \"/\".join(root.split('\\\\'))\n",
    "#     #     after_root = \"/\".join(root.split('/')[3:])\n",
    "#     #     for each_dir in dirs:\n",
    "#     #         if os.path.exists(dest+after_root+each_dir):\n",
    "#     #             continue\n",
    "#     #         else:\n",
    "#     #             os.umask(0)\n",
    "                \n",
    "#     #             os.mkdir(dest+after_root+'/'+each_dir, mode=0o777)\n",
    "        \n",
    "#     #     for file in files:\n",
    "#     #         shutil.copy(root+'/'+file, dest+after_root+'/')\n",
    "        \n",
    "#     return f'Total File : {print(len(os.listdir(dest)))}'\n",
    "\n",
    "def transferFilesToServer(src, dst):\n",
    "    print('Started copying the files...')\n",
    "    copy_tree(src, dst)\n",
    "    return 'All files copied successfully'\n",
    "\n",
    "# No '/' at end of src\n",
    "def validation_of_structure(src, participant_id):\n",
    "    \n",
    "    if os.path.exists(src+'/Video/827000'):\n",
    "        print(f'Folder Exists 827000')\n",
    "        preparing_folder(src+f'/Video/827000')\n",
    "        \n",
    "        os.mkdir(f'K:/Backup/{participant_id}')\n",
    "    elif os.path.exists(src+'/Video/984'):\n",
    "        print(f'Folder Exists 984')\n",
    "        preparing_folder(src+f'/Video/984')\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def preparing_folder(src):\n",
    "    print(f'Formatting the folder structure for Source: {src}')\n",
    "    \n",
    "    dest = \"/\".join(src.split('/')[:-1])\n",
    "    print(f'Destination Folder: {dest}')\n",
    "    \n",
    "    copy_tree(src, dest)\n",
    "    \n",
    "    # make backup and transfer files\n",
    "    copy_tree(src, 'K:/Backup/')\n",
    "    print(f'Completed the formatting of folder structure for {src}')\n",
    "\n",
    "# src = 'K:/Videos/1040/'\n",
    "# dest = 'K:/Z/1122/'\n",
    "\n",
    "source = 'K:/Videos/1001'\n",
    "destination = 'Z:/1001'\n",
    "participant_id = '1001'\n",
    " \n",
    "#  this restructure the folders to same as on server\n",
    "# 984/827000 still exists - delete manually\n",
    "validation_of_structure(source, participant_id)\n",
    "\n",
    "# transfer the file to hpc server\n",
    "transferFilesToServer(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a581b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.path.exists('K:/Z/1022/Video/2021-11-06/T084931000000.asf')\n",
    "# copy_tree(source, destination,)\n",
    "\n",
    "shutil.rmtree('k:/Z/1122/Video/2021-11-02')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a56290a",
   "metadata": {},
   "source": [
    "### Splitting of Driver files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ca1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def gender_stats_from_txtFile(file):\n",
    "    gender_list = {'male': 0, 'female':0, 'None':0}\n",
    "    with open(file) as f:\n",
    "        list_of_files = json.load(f)\n",
    "    for file in list_of_files:\n",
    "        for file, gender in file.items():\n",
    "            gender_list[gender] += 1\n",
    "    # print(f\"Total Stats: {gender_list}\")\n",
    "    return gender_list\n",
    "\n",
    "# g_rest = gender_stats_from_txtFile('gender_deepface_1013_1014.json')\n",
    "# g_1000 = gender_stats_from_txtFile('gender_deepface_1013_1014.txt')\n",
    "\n",
    "gender_stats_from_txtFile('gender_1013_1014.txt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c62bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine two json files and save it to a new file\n",
    "def combine_two_list(list1, list2):\n",
    "    return list1 + list2\n",
    "\n",
    "with open('gender_deepface_1013_1014.json') as f1:\n",
    "    l1 = json.load(f1)\n",
    "\n",
    "\n",
    "with open('gender_deepface_1013_1014.txt') as f2:\n",
    "    l2 = json.load(f2)\n",
    "\n",
    "combined_list = combine_two_list(l1, l2)\n",
    "\n",
    "with open('gender_1013_1014.txt', 'w') as f:\n",
    "    json.dump(combined_list, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from tinytag import TinyTag\n",
    "path = 'Z:/VIDEOS/1013_male/Video/'\n",
    "length = len(glob.glob(path+'**/*000.asf'))\n",
    "\n",
    "\n",
    "def get_meta(path):\n",
    "    \n",
    "    # path = os.path.join(path, file)\n",
    "    video = TinyTag.get(path)\n",
    "    video_dict = {\"album\" : video.album,\n",
    "                  \"albumartist\": video.albumartist,\n",
    "                  \"artist\" : video.artist,\n",
    "                  \"aurdio_offset\": video.audio_offset,\n",
    "                  \"bitrate\" : video.bitrate,\n",
    "                  \"comment\" : video.comment,\n",
    "                  \"composer\" : video.composer,\n",
    "                  \"disc\" : video.disc,\n",
    "                  \"disc_total\" : video.disc_total,\n",
    "                  \"duration\" : video.duration,\n",
    "                  \"filesize\" : video.filesize,\n",
    "                  \"genre\" :video.genre,\n",
    "                  \"samplerate\" : video.samplerate,\n",
    "                  \"title\" : video.title,\n",
    "                  \"track\" : video.track,\n",
    "                  \"track_total\" : video.track_total,\n",
    "                  \"year\" : video.year}\n",
    "\n",
    "    # with open('indices.json', 'w') as json_file:\n",
    "    #     json.dump(dict(video_dict), json_file)\n",
    "    return video_dict\n",
    "\n",
    "def get_duration_modified(path):\n",
    "    f_dict = {}\n",
    "    # Glob - getting list of files\n",
    "    counter = 0\n",
    "    total_fileList = len(glob.glob(path+'**/*000.asf'))\n",
    "    \n",
    "    for file in glob.glob(path+'**/*000.asf'):\n",
    "        dur = 0\n",
    "        file = file.replace('\\\\', '/')\n",
    "        metas = get_meta(file)\n",
    "        try:\n",
    "            if metas['duration'] < 10000:\n",
    "                # print(f'Duration: {metas[\"duration\"]}')\n",
    "                dur += int(metas['duration'])\n",
    "        except:\n",
    "            dur += 0\n",
    "            print(f'get_duration()::Error During Processing: {file}')\n",
    "        dir_split = \"\".join(file.split(\"/\")[-1].split(\"-\"))\n",
    "        print(f'Processing : {counter}/{total_fileList}, File: {dir_split}')\n",
    "        counter += 1\n",
    "        print(f_dict)\n",
    "        f_dict[dir_split] = dur\n",
    "    \n",
    "    return f_dict\n",
    "\n",
    "get_duration_modified(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ecbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "telematic_data = pd.read_excel(\"./Data _for_Integration.xlsx\")\n",
    "telematic_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab244726",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in telematic_data.iterrows():\n",
    "    print(row[1]['PID'])\n",
    "    print(row[1]['trip start'])\n",
    "    print(row[1]['trip end'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "telematic_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9175b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "telematic_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d916cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data = [\n",
    "#     {\"id\": \"1001\", \"day\": \"20210920\", \"duration\": 35.86, \"yawn\": {\"total\": 0, \"timestamp\": []}, \"smoking\": {\"total\": 1, \"timestamp\": [\"20210920103721\"]}, \"mobilephone\": {\"total\": 0, \"timestamp\": []}, \"distraction\": {\"total\": 0, \"timestamp\": []}, \"eyeclosing\": {\"total\": 0, \"timestamp\": []}, \"crossinglane\": {\"total\": 0, \"timestamp\": []}, \"nearcollision\": {\"total\": 0, \"timestamp\": []}, \"stopsign\": {\"total\": 0, \"timestamp\": []}, \"redlight\": {\"total\": 0, \"timestamp\": []}, \"pedestrian\": {\"total\": 0, \"timestamp\": []}},\n",
    "#     {\"id\": \"1001\", \"day\": \"20210921\", \"duration\": 39.76, \"yawn\": {\"total\": 0, \"timestamp\": []}, \"smoking\": {\"total\": 1, \"timestamp\": [\"20210921103001\"]}, \"mobilephone\": {\"total\": 0, \"timestamp\": []}, \"distraction\": {\"total\": 0, \"timestamp\": []}, \"eyeclosing\": {\"total\": 0, \"timestamp\": []}, \"crossinglane\": {\"total\": 0, \"timestamp\": []}, \"nearcollision\": {\"total\": 0, \"timestamp\": []}, \"stopsign\": {\"total\": 0, \"timestamp\": []}, \"redlight\": {\"total\": 0, \"timestamp\": []}, \"pedestrian\": {\"total\": 0, \"timestamp\": []}},\n",
    "#     {\"id\": \"1001\", \"day\": \"20230401\", \"duration\": 71.53, \"yawn\": {\"total\": 0, \"timestamp\": []}, \"smoking\": {\"total\": 14, \"timestamp\": [\"20230401122415\", \"20230401122734\", \"20230401123008\", \"20230401123037\", \"20230401123201\", \"20230401124432\", \"20230401125501\", \"20230401183050\", \"20230401183141\", \"20230401183320\", \"20230401183333\", \"20230401184319\", \"20230401184722\", \"20230401190215\"]}, \"mobilephone\": {\"total\": 0, \"timestamp\": []}, \"distraction\": {\"total\": 0, \"timestamp\": []}, \"eyeclosing\": {\"total\": 1, \"timestamp\": [\"20230401185357\"]}, \"crossinglane\": {\"total\": 12, \"timestamp\": [\"20230401124329\", \"20230401124528\", \"20230401124012\", \"20230401125254\", \"20230401183051\", \"20230401183728\", \"20230401183837\", \"20230401185150\", \"20230401185438\", \"20230401185453\", \"20230401183557\", \"20230401185005\"]}, \"nearcollision\": {\"total\": 0, \"timestamp\": []}, \"stopsign\": {\"total\": 0, \"timestamp\": []}, \"redlight\": {\"total\": 0, \"timestamp\": []}, \"pedestrian\": {\"total\": 0, \"timestamp\": []}}\n",
    "# ]\n",
    "with open('data_storage.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "flat_data = []\n",
    "\n",
    "for entry in data['data']:\n",
    "    id_value = entry[\"id\"]\n",
    "    day = entry[\"day\"]\n",
    "    for key, value in entry.items():\n",
    "        if key not in [\"id\", \"day\", \"duration\"]:\n",
    "            if isinstance(value, dict):\n",
    "                total = value[\"total\"]\n",
    "                timestamps = value[\"timestamp\"]\n",
    "                for timestamp in timestamps:\n",
    "                    flat_data.append({\"id\": id_value, \"day\": day, \"type\": key, \"timestamp\": timestamp})\n",
    "\n",
    "df = pd.DataFrame(flat_data)\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91155678",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('video_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb452c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime\n",
    "# Load telematics data\n",
    "telematics_data = pd.read_excel('Data_for_Integration.xlsx')\n",
    "\n",
    "# Convert trip start and trip end columns to datetime\n",
    "telematics_data['trip start'] = pd.to_datetime(telematics_data['trip start'])\n",
    "telematics_data['trip end'] = pd.to_datetime(telematics_data['trip end'])\n",
    "\n",
    "# Load video data from Excel file\n",
    "video_data = pd.read_excel('video_data.xlsx')\n",
    "\n",
    "# Convert the 'timestamp' column to datetime from the 'yyyymmddhhmmss' format\n",
    "video_data['timestamp'] = pd.to_datetime(video_data['timestamp'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Merge based on PID, day, and timestamp\n",
    "merged_data = pd.merge(telematics_data, video_data, how='left', left_on=['PID', 'trip start', 'trip end'], right_on=['id', 'day', 'timestamp'])\n",
    "\n",
    "# Drop redundant columns\n",
    "merged_data = merged_data.drop(['id', 'day', 'timestamp'], axis=1)\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv('merged_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load telematics data\n",
    "telematics_data = pd.read_excel('Data_for_Integration.xlsx')\n",
    "\n",
    "# Convert trip start and trip end columns to datetime\n",
    "telematics_data['trip start'] = pd.to_datetime(telematics_data['trip start'])\n",
    "telematics_data['trip end'] = pd.to_datetime(telematics_data['trip end'])\n",
    "\n",
    "# Load video data from Excel file\n",
    "video_data = pd.read_excel('video_data.xlsx')\n",
    "\n",
    "# Convert the 'timestamp' column to datetime from the 'yyyymmddhhmmss' format\n",
    "video_data['timestamp'] = pd.to_datetime(video_data['timestamp'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Merge based on 'id' and 'day'\n",
    "merged_data = pd.merge(telematics_data, video_data, how='left', left_on=['PID', 'trip start', 'trip end'], right_on=['id', 'day'])\n",
    "\n",
    "# Filter the merged data based on timestamp\n",
    "merged_data = merged_data[(merged_data['timestamp'] >= merged_data['trip start']) & (merged_data['timestamp'] <= merged_data['trip end'])]\n",
    "\n",
    "# Drop redundant columns\n",
    "merged_data = merged_data.drop(['timestamp', 'day', 'id'], axis=1)\n",
    "\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv('merged_data.csv', index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c106316",
   "metadata": {},
   "outputs": [],
   "source": [
    "telematics_data = pd.read_excel('Data_for_Integration.xlsx')\n",
    "# Convert 'trip start' and 'trip end' columns to datetime\n",
    "telematics_data['trip start'] = pd.to_datetime(telematics_data['trip start'])\n",
    "telematics_data['trip end'] = pd.to_datetime(telematics_data['trip end'])\n",
    "\n",
    "# Convert to the desired format\n",
    "telematics_data['trip start formatted'] = telematics_data['trip start'].apply(lambda x: x.strftime('%Y%m%d%H%M%S'))\n",
    "telematics_data['trip end formatted'] = telematics_data['trip end'].apply(lambda x: x.strftime('%Y%m%d%H%M%S'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a346bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "telematics_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc11d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "telematics_data.to_excel('telematic_data_formatted.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561c5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load video data from Excel file\n",
    "video_data = pd.read_excel('video_data.xlsx')\n",
    "\n",
    "# Convert the 'timestamp' column to datetime from the 'yyyymmddhhmmss' format\n",
    "video_data['timestamp'] = pd.to_datetime(video_data['timestamp'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Merge based on 'id' and 'day'\n",
    "merged_data = pd.merge(telematics_data, video_data, how='left', left_on=['PID', 'trip start formatted', 'trip end formatted'], right_on=['id', 'day', 'timestamp'])\n",
    "\n",
    "# Filter the merged data based on timestamp\n",
    "filtered_data = merged_data[(merged_data['timestamp'] >= merged_data['trip start']) & (merged_data['timestamp'] <= merged_data['trip end'])]\n",
    "\n",
    "# Aggregate data (you can use your own aggregation functions)\n",
    "aggregated_data = filtered_data.groupby(['PID', 'trip start', 'trip end'], as_index=False).agg({\n",
    "    'smoking': 'sum',\n",
    "    'mobilephone': 'sum',\n",
    "    # Add other columns and aggregation functions as needed\n",
    "})\n",
    "\n",
    "# # Merge the aggregated data back to telematics_data\n",
    "# final_data = pd.merge(telematics_data, aggregated_data, how='left', on=['PID', 'trip start', 'trip end'])\n",
    "\n",
    "# # Save the final merged data to a new CSV file\n",
    "# final_data.to_csv('final_merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e3a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a4862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "video_data = pd.read_excel('video_data.xlsx')\n",
    "trip_data = pd.read_excel('telematic_data_formatted.xlsx')\n",
    "\n",
    "# Convert timestamp columns to datetime format\n",
    "video_data['timestamp'] = pd.to_datetime(video_data['timestamp'], format='%Y%m%d%H%M%S')\n",
    "trip_data['trip start formatted'] = pd.to_datetime(trip_data['trip start formatted'], format='%Y%m%d%H%M%S')\n",
    "trip_data['trip end formatted'] = pd.to_datetime(trip_data['trip end formatted'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Create a function to check if a timestamp is within a given interval\n",
    "def check_interval(row):\n",
    "    return video_data[(video_data['timestamp'] >= row['trip start formatted']) & (video_data['timestamp'] <= row['trip end formatted'])].shape[0]\n",
    "\n",
    "# Apply the function to create a new column in trip_data\n",
    "trip_data['count_per_type'] = trip_data.apply(check_interval, axis=1)\n",
    "\n",
    "# Merge the two datasets based on 'id'\n",
    "merged_data = pd.merge(video_data, trip_data, left_on='id', right_on='PID')\n",
    "\n",
    "\n",
    "merged_data.head()\n",
    "# Save the merged data to a new CSV file\n",
    "merged_data.to_csv('merged_data2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd806045",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load telematic and video data\n",
    "telematic_data = pd.read_csv(\"telematicdatafile.csv\")\n",
    "video_data = pd.read_csv(\"videofiledata.csv\")\n",
    "\n",
    "# Merge data frames\n",
    "merged_data = pd.merge(telematic_data, video_data, left_on=['trip start formatted', 'trip end formatted'], right_on=['timestamp'])\n",
    "\n",
    "# Convert timestamps to datetime format\n",
    "merged_data['trip_start'] = pd.to_datetime(merged_data['trip start formatted'], format='%Y%m%d%H%M%S')\n",
    "merged_data['trip_end'] = pd.to_datetime(merged_data['trip end formatted'], format='%Y%m%d%H%M%S')\n",
    "merged_data['timestamp'] = pd.to_datetime(merged_data['timestamp'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Function to count occurrences within a given interval\n",
    "def count_occurrences(row):\n",
    "    interval_data = merged_data[(merged_data['timestamp'] >= row['trip_start']) & (merged_data['timestamp'] <= row['trip_end'])]\n",
    "    occurrences = interval_data['type'].value_counts()\n",
    "    return occurrences\n",
    "\n",
    "# Apply the function to create new columns\n",
    "occurrence_columns = ['nearcollision', 'crossinglane', 'smoking']  # Add more types as needed\n",
    "for col in occurrence_columns:\n",
    "    telematic_data[col + '_count'] = telematic_data.apply(count_occurrences, axis=1)[col]\n",
    "\n",
    "# Save the updated telematic data\n",
    "telematic_data.to_csv(\"updated_telematic_data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86d5bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip_data['count_per_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ff67cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "video_data = pd.read_excel('video_data.xlsx')\n",
    "trip_data = pd.read_excel('telematic_data_formatted.xlsx')\n",
    "\n",
    "# Convert timestamp columns to datetime format\n",
    "video_data['timestamp'] = pd.to_datetime(video_data['timestamp'], format='%Y%m%d%H%M%S')\n",
    "trip_data['trip start formatted'] = pd.to_datetime(trip_data['trip start formatted'], format='%Y%m%d%H%M%S')\n",
    "trip_data['trip end formatted'] = pd.to_datetime(trip_data['trip end formatted'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Create a function to check if a timestamp is within a given interval\n",
    "def check_interval(row):\n",
    "    total = video_data[(video_data['timestamp'] >= row['trip start formatted']) & (video_data['timestamp'] <= row['trip end formatted'])]\n",
    "    # return total.shape[0]\n",
    "    return total['type'].value_counts().to_dict()\n",
    "\n",
    "# Apply the function to create a new column in trip_data\n",
    "trip_data['count_per_type'] = trip_data.apply(check_interval, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c1ca2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{}\n",
      "{'crossinglane': 17, 'nearcollision': 14}\n",
      "{'nearcollision': 19, 'crossinglane': 3, 'smoking': 1}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming trip_data is your telematic data DataFrame\n",
    "# Load telematic and video data into DataFrames\n",
    "# Read the CSV files\n",
    "# video_data = pd.read_excel('video_data.xlsx')\n",
    "# telematic_data = pd.read_excel('telematic_data_formatted.xlsx')\n",
    "video_data = pd.read_csv('videofiledata.csv')\n",
    "telematic_data = pd.read_csv('telematicdatafile.csv')\n",
    "\n",
    "# Merge DataFrames based on timestamp\n",
    "merged_data = pd.merge(telematic_data, video_data, left_on=\"trip start formatted\", right_on=\"timestamp\", how=\"left\")\n",
    "\n",
    "# Create a function to check if a timestamp is within a given interval\n",
    "def check_interval(row):\n",
    "    total = video_data[(video_data['id'] == row['PID']) & (video_data['timestamp'] >= row['trip start formatted']) & (video_data['timestamp'] <= row['trip end formatted'])]\n",
    "    print(total['type'].value_counts().to_dict())\n",
    "    return total['type'].value_counts().to_dict()\n",
    "\n",
    "# Apply the function to create a new column in telematic_data\n",
    "telematic_data['count_per_type'] = telematic_data.apply(check_interval, axis=1)\n",
    "\n",
    "# Expand the dictionary into separate columns\n",
    "count_per_type_df = telematic_data['count_per_type'].apply(pd.Series)\n",
    "\n",
    "# Merge the expanded data back to the original telematic DataFrame\n",
    "final_data = pd.concat([telematic_data, count_per_type_df], axis=1)\n",
    "\n",
    "# Drop the original 'count_per_type' column if needed\n",
    "final_data = final_data.drop('count_per_type', axis=1)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "final_data = final_data.fillna(0)\n",
    "\n",
    "# Print or use the final DataFrame as needed\n",
    "# print(final_data)\n",
    "\n",
    "final_data.to_excel('final_data.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4719891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV files\n",
    "video_data = pd.read_csv('videofiledata.csv')\n",
    "trip_data = pd.read_csv('telematicdatafile.csv')\n",
    "\n",
    "# Convert timestamp columns to datetime format\n",
    "video_data['timestamp'] = pd.to_datetime(video_data['timestamp'], format='%Y%m%d%H%M%S')\n",
    "trip_data['trip start formatted'] = pd.to_datetime(trip_data['trip start formatted'], format='%Y%m%d%H%M%S')\n",
    "trip_data['trip end formatted'] = pd.to_datetime(trip_data['trip end formatted'], format='%Y%m%d%H%M%S')\n",
    "\n",
    "# Create a function to check if a timestamp is within a given interval\n",
    "def check_interval(row):\n",
    "    total = video_data[(video_data['timestamp'] >= row['trip start formatted']) & (video_data['timestamp'] <= row['trip end formatted'])]\n",
    "    # print(total['type'].value_counts())\n",
    "    print(total['type'].value_counts().to_dict())\n",
    "    return total.shape[0]\n",
    "\n",
    "# Apply the function to create a new column in trip_data\n",
    "trip_data['count_per_type'] = trip_data.apply(check_interval, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6bf879",
   "metadata": {},
   "source": [
    "### FacenetPytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7755e544",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tanve\\PycharmProjects\\Driver_indices\\.venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "593c13b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data/test_images'\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 8\n",
    "workers = 0 if os.name == 'nt' else 8\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23097b5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MTCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m mtcnn \u001b[38;5;241m=\u001b[39m \u001b[43mMTCNN\u001b[49m(\n\u001b[0;32m      2\u001b[0m     image_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m160\u001b[39m, margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, min_face_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m      3\u001b[0m     thresholds\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.6\u001b[39m, \u001b[38;5;241m0.7\u001b[39m, \u001b[38;5;241m0.7\u001b[39m], factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.709\u001b[39m, post_process\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      4\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice\n\u001b[0;32m      5\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MTCNN' is not defined"
     ]
    }
   ],
   "source": [
    "mtcnn = MTCNN(\n",
    "    image_size=160, margin=0, min_face_size=20,\n",
    "    thresholds=[0.6, 0.7, 0.7], factor=0.709, post_process=True,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "dataset = datasets.ImageFolder(data_dir, transform=transforms.Resize((512, 512)))\n",
    "dataset.samples = [\n",
    "    (p, p.replace(data_dir, data_dir + '_cropped'))\n",
    "        for p, _ in dataset.samples\n",
    "]\n",
    "        \n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=training.collate_pil\n",
    ")\n",
    "\n",
    "for i, (x, y) in enumerate(loader):\n",
    "    mtcnn(x, save_path=y)\n",
    "    print('\\rBatch {} of {}'.format(i + 1, len(loader)), end='')\n",
    "    \n",
    "# Remove mtcnn to reduce GPU memory usage\n",
    "del mtcnn\n",
    "\n",
    "resnet = InceptionResnetV1(\n",
    "    classify=True,\n",
    "    pretrained='vggface2',\n",
    "    num_classes=len(dataset.class_to_idx)\n",
    ").to(device)\n",
    "\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.001)\n",
    "scheduler = MultiStepLR(optimizer, [5, 10])\n",
    "\n",
    "trans = transforms.Compose([\n",
    "    np.float32,\n",
    "    transforms.ToTensor(),\n",
    "    fixed_image_standardization\n",
    "])\n",
    "dataset = datasets.ImageFolder(data_dir + '_cropped', transform=trans)\n",
    "img_inds = np.arange(len(dataset))\n",
    "np.random.shuffle(img_inds)\n",
    "train_inds = img_inds[:int(0.8 * len(img_inds))]\n",
    "val_inds = img_inds[int(0.8 * len(img_inds)):]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(train_inds)\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers=workers,\n",
    "    batch_size=batch_size,\n",
    "    sampler=SubsetRandomSampler(val_inds)\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "metrics = {\n",
    "    'fps': training.BatchTimer(),\n",
    "    'acc': training.accuracy\n",
    "}\n",
    "\n",
    "writer = SummaryWriter()\n",
    "writer.iteration, writer.interval = 0, 10\n",
    "\n",
    "print('\\n\\nInitial')\n",
    "print('-' * 10)\n",
    "resnet.eval()\n",
    "training.pass_epoch(\n",
    "    resnet, loss_fn, val_loader,\n",
    "    batch_metrics=metrics, show_running=True, device=device,\n",
    "    writer=writer\n",
    ")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    resnet.train()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, train_loader, optimizer, scheduler,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "    resnet.eval()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, val_loader,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        writer=writer\n",
    "    )\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4954b42",
   "metadata": {},
   "source": [
    "write files name to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571ef2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Gender         ID        Date               Filename\n",
      "0      female  1003-1004  2021-11-30  T005614000000.asf.jpg\n",
      "1      female  1003-1004  2021-12-03  T034933000000.asf.jpg\n",
      "2      female  1003-1004  2021-12-12  T043938000000.asf.jpg\n",
      "3      female  1003-1004  2021-12-16  T024910000000.asf.jpg\n",
      "4      female  1003-1004  2021-12-17  T233412000000.asf.jpg\n",
      "...       ...        ...         ...                    ...\n",
      "13101    male  1043-1044  2022-04-15  T235628000000.asf.jpg\n",
      "13102    male  1043-1044  2022-04-16  T003659000000.asf.jpg\n",
      "13103    male  1043-1044  2022-04-16  T004507000000.asf.jpg\n",
      "13104    male  1043-1044  2022-04-16  T005514000000.asf.jpg\n",
      "13105    male  1043-1044  2022-04-16  T005800000000.asf.jpg\n",
      "\n",
      "[13106 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = []\n",
    "for files in os.listdir('./GenderData/Gender_images/'):\n",
    "    parts = files.split('_')\n",
    "    # print(parts)\n",
    "    gender = parts[0]\n",
    "    ids = \"-\".join(parts[1:3])\n",
    "    date = parts[3]\n",
    "    filename = parts[4]\n",
    "    data.append([gender, ids, date, filename])\n",
    "\n",
    "# Creating pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['Gender', 'ID', 'Date', 'Filename'])\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2bdcd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./GenderData/gender_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b69b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Driver_indices-DsFqzCcB",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "5dd92fb0320f6de981139fad36e905da6c43d5f0ca47ced40c32e9f3f500edc2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
